import json
import ast
from pathlib import Path
from collections import Counter
from typing import Set, List, Dict, Any

def load_data(file_path: str = "data/data.json", limit: int = None) -> List[Dict]:
    """åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œå¯é™åˆ¶åŠ è½½çš„ä»“åº“æ•°é‡"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æå–ä»“åº“æ•°æ®ï¼ˆå¤„ç†ç‰¹æ®Šçš„JSONæ ¼å¼ï¼‰
        repositories = []
        for key in data:
            repositories.extend(data[key])
        
        # å¦‚æœæŒ‡å®šäº†é™åˆ¶æ•°é‡ï¼Œåˆ™æˆªå–å‰limitä¸ª
        if limit is not None:
            repositories = repositories[:limit]
            print(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}ï¼Œå·²é™åˆ¶ä¸ºå‰ {limit} ä¸ªä»“åº“")
        else:
            print(f"æˆåŠŸåŠ è½½æ•°æ®æ–‡ä»¶: {file_path}ï¼Œå…± {len(repositories)} ä¸ªä»“åº“")
        
        return repositories
    except Exception as e:
        print(f"åŠ è½½æ•°æ®æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return []

def parse_topics(topics_str: str) -> Set[str]:
    """è§£ætopicså­—ç¬¦ä¸²ä¸ºé›†åˆ"""
    if not topics_str or topics_str == '':
        return set()
    
    try:
        # ä½¿ç”¨ast.literal_evalå®‰å…¨è§£æPythonå­—é¢é‡
        topics = ast.literal_eval(topics_str)
        if isinstance(topics, set):
            return topics
        elif isinstance(topics, list):
            return set(topics)
        else:
            return {str(topics)}
    except (SyntaxError, ValueError):
        # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›ç©ºé›†åˆ
        print(f"è§£ætopicså¤±è´¥: {topics_str}")
        return set()

def analyze_dataset(repositories: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææ•°æ®é›†çš„åŸºæœ¬ä¿¡æ¯"""
    stats = {
        'total_repositories': 0,
        'repositories_with_topics': 0,
        'repositories_without_topics': 0,
        'total_topic_occurrences': 0,
        'unique_topics': set(),
        'topic_frequency': Counter(),
        'repositories_with_description': 0,
        'repositories_with_readme': 0,
        'repositories_with_both_desc_readme': 0,
        'empty_descriptions': 0,
        'empty_readmes': 0,
        'avg_topics_per_repo': 0,
        'max_topics_per_repo': 0,
        'min_topics_per_repo': float('inf'),
        'repos_by_topic_count': Counter(),
        'top_topics': [],
        'sample_repositories': []
    }
    
    all_topics = []
    topics_per_repo = []
    
    for repo in repositories:
        stats['total_repositories'] += 1
        
        # åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
        description = repo.get('a.description', '')
        readme = repo.get('a.readme_text', '')
        topics_str = repo.get('a.topics', '')
        repo_name = repo.get('b.repo_name', 'Unknown')
        
        # æè¿°å’ŒREADMEç»Ÿè®¡
        if description and description.strip():
            stats['repositories_with_description'] += 1
        else:
            stats['empty_descriptions'] += 1
            
        if readme and readme.strip():
            stats['repositories_with_readme'] += 1
        else:
            stats['empty_readmes'] += 1
            
        if (description and description.strip()) and (readme and readme.strip()):
            stats['repositories_with_both_desc_readme'] += 1
        
        # Topicsç»Ÿè®¡
        topics = parse_topics(topics_str)
        
        if topics:
            stats['repositories_with_topics'] += 1
            topic_count = len(topics)
            topics_per_repo.append(topic_count)
            stats['repos_by_topic_count'][topic_count] += 1
            
            # æ›´æ–°æœ€å¤§æœ€å°topicsæ•°é‡
            stats['max_topics_per_repo'] = max(stats['max_topics_per_repo'], topic_count)
            stats['min_topics_per_repo'] = min(stats['min_topics_per_repo'], topic_count)
            
            # æ”¶é›†æ‰€æœ‰topics
            all_topics.extend(topics)
            stats['unique_topics'].update(topics)
            
            # æ›´æ–°topicé¢‘ç‡
            for topic in topics:
                stats['topic_frequency'][topic] += 1
        else:
            stats['repositories_without_topics'] += 1
        
        # æ”¶é›†æ ·æœ¬ä»“åº“ä¿¡æ¯ï¼ˆå‰10ä¸ªï¼‰
        if len(stats['sample_repositories']) < 10:
            stats['sample_repositories'].append({
                'repo_name': repo_name,
                'description': description[:100] + '...' if len(description) > 100 else description,
                'topics_count': len(topics),
                'topics': list(topics)[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªtopics
            })
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    stats['total_topic_occurrences'] = len(all_topics)
    stats['unique_topics_count'] = len(stats['unique_topics'])
    
    if stats['repositories_with_topics'] > 0:
        stats['avg_topics_per_repo'] = sum(topics_per_repo) / len(topics_per_repo)
    
    if stats['min_topics_per_repo'] == float('inf'):
        stats['min_topics_per_repo'] = 0
    
    # è·å–æœ€çƒ­é—¨çš„topics
    stats['top_topics'] = stats['topic_frequency'].most_common(30)
    
    # è½¬æ¢unique_topicsä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
    stats['unique_topics'] = list(stats['unique_topics'])
    
    return stats

def print_statistics(stats: Dict[str, Any], limit: int = None):
    """æ‰“å°ç»Ÿè®¡ç»“æœ"""
    limit_info = f"ï¼ˆå‰ {limit} ä¸ªä»“åº“ï¼‰" if limit is not None else ""
    
    print("\n" + "="*80)
    print(f"GitHub ä»“åº“æ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š{limit_info}")
    print("="*80)
    
    print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»ä»“åº“æ•°é‡: {stats['total_repositories']:,}")
    print(f"  åŒ…å«topicsçš„ä»“åº“: {stats['repositories_with_topics']:,} ({stats['repositories_with_topics']/stats['total_repositories']*100:.1f}%)")
    print(f"  ä¸åŒ…å«topicsçš„ä»“åº“: {stats['repositories_without_topics']:,} ({stats['repositories_without_topics']/stats['total_repositories']*100:.1f}%)")
    
    print(f"\nğŸ·ï¸ Topicsç»Ÿè®¡:")
    print(f"  Topicæ ‡ç­¾æ€»å‡ºç°æ¬¡æ•°: {stats['total_topic_occurrences']:,}")
    print(f"  ä¸é‡å¤topicæ ‡ç­¾æ€»æ•°: {stats['unique_topics_count']:,}")
    print(f"  å¹³å‡æ¯ä¸ªä»“åº“çš„topicsæ•°é‡: {stats['avg_topics_per_repo']:.2f}")
    print(f"  å•ä¸ªä»“åº“æœ€å¤štopicsæ•°é‡: {stats['max_topics_per_repo']}")
    print(f"  å•ä¸ªä»“åº“æœ€å°‘topicsæ•°é‡: {stats['min_topics_per_repo']}")
    
    print(f"\nğŸ“ å†…å®¹ç»Ÿè®¡:")
    print(f"  åŒ…å«æè¿°çš„ä»“åº“: {stats['repositories_with_description']:,} ({stats['repositories_with_description']/stats['total_repositories']*100:.1f}%)")
    print(f"  åŒ…å«READMEçš„ä»“åº“: {stats['repositories_with_readme']:,} ({stats['repositories_with_readme']/stats['total_repositories']*100:.1f}%)")
    print(f"  åŒæ—¶åŒ…å«æè¿°å’ŒREADMEçš„ä»“åº“: {stats['repositories_with_both_desc_readme']:,} ({stats['repositories_with_both_desc_readme']/stats['total_repositories']*100:.1f}%)")
    print(f"  ç©ºæè¿°çš„ä»“åº“: {stats['empty_descriptions']:,}")
    print(f"  ç©ºREADMEçš„ä»“åº“: {stats['empty_readmes']:,}")
    
    print(f"\nğŸ”¥ æœ€çƒ­é—¨çš„30ä¸ªTopics:")
    for i, (topic, count) in enumerate(stats['top_topics'], 1):
        percentage = count / stats['repositories_with_topics'] * 100
        print(f"  {i:2d}. {topic:<25} {count:4d} æ¬¡ ({percentage:5.1f}%)")
    
    print(f"\nğŸ“ˆ æŒ‰Topicsæ•°é‡åˆ†å¸ƒçš„ä»“åº“:")
    sorted_topic_counts = sorted(stats['repos_by_topic_count'].items())
    for topic_count, repo_count in sorted_topic_counts[:15]:  # æ˜¾ç¤ºå‰15ä¸ª
        percentage = repo_count / stats['total_repositories'] * 100
        print(f"  {topic_count:2d} ä¸ªtopics: {repo_count:4d} ä¸ªä»“åº“ ({percentage:5.1f}%)")
    
    print(f"\nğŸ“‹ æ ·æœ¬ä»“åº“ä¿¡æ¯:")
    for i, repo in enumerate(stats['sample_repositories'], 1):
        print(f"  {i:2d}. {repo['repo_name']}")
        print(f"      æè¿°: {repo['description']}")
        print(f"      Topicsæ•°é‡: {repo['topics_count']}")
        print(f"      Topicsç¤ºä¾‹: {', '.join(repo['topics'])}")
        print()

def save_statistics(stats: Dict[str, Any], output_file: str = "data_statistics.json"):
    """ä¿å­˜ç»Ÿè®¡ç»“æœåˆ°æ–‡ä»¶"""
    # å‡†å¤‡å¯åºåˆ—åŒ–çš„æ•°æ®
    serializable_stats = stats.copy()
    
    # è½¬æ¢Counterå¯¹è±¡ä¸ºå­—å…¸
    serializable_stats['topic_frequency'] = dict(stats['topic_frequency'])
    serializable_stats['repos_by_topic_count'] = dict(stats['repos_by_topic_count'])
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_stats, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    except Exception as e:
        print(f"ä¿å­˜ç»Ÿè®¡ç»“æœæ—¶å‡ºé”™: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹åˆ†æGitHubä»“åº“æ•°æ®é›†...")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    data_file = "data/data.json"
    if not Path(data_file).exists():
        print(f"é”™è¯¯: æ•°æ®æ–‡ä»¶ '{data_file}' ä¸å­˜åœ¨")
        return
    
    # åŠ è½½å‰3000ä¸ªä»“åº“çš„æ•°æ®
    repositories = load_data(data_file, limit=3000)
    if not repositories:
        print("æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆæ•°æ®")
        return
    
    # åˆ†ææ•°æ®
    print("æ­£åœ¨åˆ†ææ•°æ®...")
    stats = analyze_dataset(repositories)
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print_statistics(stats, limit=3000)
    
    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    save_statistics(stats, "data_statistics_top3000.json")
    
    print("\nâœ… æ•°æ®åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()